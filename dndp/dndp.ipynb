{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook for Discrete Network Design Problem (DNDP)\n",
    "\n",
    "This self-contained notebook contains all the code to reproduce the results for the DNDP.  For DNDP, the implementation is more straightforward as a single model is trained for the unchanged network. In contrast, KP, CNP, and DRP use trained models across instances with variable parameters.  This assumption for DNDP allows simpler models to be used, i.e., feed-forward networks and gradient-boosted trees, and simpler surrogate models. his assumption for DNDP allow simplier models to be used, i.e., feed-forward networks and gradient boosted trees, and simplier surrogate models.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lDTlS286VcMb"
   },
   "source": [
    "### DNDP info\n",
    "\n",
    "$t(x_a)=T_a\\Bigg(1+B_a\\bigg(\\dfrac{x}{c_a}\\bigg)^4\\Bigg)$\n",
    "\n",
    "Follower objective: $T_a x_a + \\frac{T_a B_a}{5c_a^4}x^5$\n",
    "\n",
    "$B_a$ is alpha\n",
    "\n",
    "$e_a$ is beta\n",
    "\n",
    "$T_a$ is fftt\n",
    "\n",
    "How are new link endpoints, capacities, and costs chosen?\n",
    "\n",
    "How are leader budgets chosen? Sum of costs x  25/50/75%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2PqQPawCPGcp",
    "outputId": "074d578c-6d7a-4965-d8e0-2d3287d57b26"
   },
   "outputs": [],
   "source": [
    "fig_path = 'figs/'\n",
    "data_path = 'data/'\n",
    "result_path = 'results/'\n",
    "instance_path = 'Instances/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hFfaMrTj0oaV",
    "outputId": "2c4cba48-7529-471c-bc25-3ac8b25ed4ec"
   },
   "outputs": [],
   "source": [
    "import gurobipy as gp  \n",
    "\n",
    "import time\n",
    "import collections\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import itertools\n",
    "from scipy import stats\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pyomo.environ as pyo\n",
    "from pyomo.environ import SolverFactory\n",
    "from pyomo.opt import SolverStatus, TerminationCondition\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "import gurobipy as gp\n",
    "from gurobi_ml import add_predictor_constr\n",
    "from gurobi_ml.sklearn import add_gradient_boosting_regressor_constr\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Instance Reading + Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JlcM5J2lZqlo"
   },
   "outputs": [],
   "source": [
    "def read_instance(net,NDP,B_prop,m,scal_time,scal_flow,timelimit,my_edges=None,my_data=None):\n",
    "\n",
    "    #---read network and instance data in extended TNTP format\n",
    "    nodes,links,capa,fftt,alpha,beta,links2,cost = read_network_data(net,NDP)\n",
    "\n",
    "    #---read trip data in TNTP format\n",
    "    OD,orig,dest = read_trip_data(net)\n",
    "\n",
    "    N = list(nodes)\n",
    "    A1 = list(links)\n",
    "    if my_edges is None:\n",
    "        A2 = list(links2)\n",
    "    else:\n",
    "        A2 = my_edges\n",
    "        fftt = my_data['fftt']\n",
    "        alpha = my_data['alpha_raw']\n",
    "        beta = my_data['beta']\n",
    "        e = my_data['exp']\n",
    "        capa = my_data['capa']\n",
    "        cost = my_data['cost']\n",
    "\n",
    "    A = A1+A2\n",
    "    O = list(orig)\n",
    "    D = list(dest)\n",
    "\n",
    "    #---create node-destination demand matrix\n",
    "    TD = 0\n",
    "    d = {(i,s):0 for i in N for s in D}\n",
    "    for r in O:\n",
    "        for s in D:\n",
    "            d[r,s] = OD[r,s]\n",
    "            TD += d[r,s]\n",
    "    for s in D:\n",
    "        d[s,s] = - sum(d[j,s] for j in O)\n",
    "\n",
    "    #---create link delay function parameters from TNTP data\n",
    "    #---link delay functional form: t[i,j] = T[i,j] + c[i,j]*(x**exp[i,j])\n",
    "    T = {(i,j):fftt[i,j] for (i,j) in A}\n",
    "    c = {(i,j):fftt[i,j]*alpha[i,j]/(capa[i,j]**beta[i,j]) for (i,j) in A}\n",
    "    e = {(i,j):beta[i,j] for (i,j) in A}\n",
    "\n",
    "    #---create link cost matrix and budget\n",
    "    g = {(i,j):cost[i,j] for (i,j) in A2}\n",
    "    TC = sum(g[i,j] for (i,j) in A2)\n",
    "    B = B_prop*TC\n",
    "\n",
    "    #---link delay function linear approximation using m uniform segments\n",
    "    #---maximum link flow is instance-specific: value is calibrated for Sioux Falls under base demand\n",
    "    Mflow = 1e5*scal_flow\n",
    "    V = set([i for i in range(0,m+1)])\n",
    "    a = {(i,j,v):float() for (i,j) in A for v in V}\n",
    "    for (i,j) in A:\n",
    "        cnt = 0\n",
    "        step = Mflow/(len(V)-1)\n",
    "        for v in V:\n",
    "            a[i,j,v] = cnt*step\n",
    "            cnt += 1\n",
    "\n",
    "    #---time and flow scaling\n",
    "    #---big-M coefficient is instance-specific: value is calibrated for Sioux Falls under base demand\n",
    "    Mtt = 1e3*scal_time\n",
    "    for (i,j) in A:\n",
    "        T[i,j] = T[i,j]*scal_time\n",
    "        c[i,j] = c[i,j]*scal_time/(scal_flow**e[i,j])\n",
    "    for i in N:\n",
    "        for s in D:\n",
    "            d[i,s] = d[i,s]*scal_flow\n",
    "\n",
    "    print('Instance',)\n",
    "    print('Total scaled demand',TD*scal_flow)\n",
    "    print('Total cost',TC,'Budget',B)\n",
    "\n",
    "    data = {'nodes':N,'links1':A1,'links2':A2,'links':A,'orig':O,'dest':D,'fftt':T,'coef':c,'exp':e,\n",
    "            'approx':V,'alpha':a,'cost':g,'demand':d,'budget':B,'Mflow':Mflow,'Mtt':Mtt,'timelimit':timelimit,\n",
    "            'capa':capa, 'alpha_raw':alpha, 'beta':beta}\n",
    "    return data\n",
    "\n",
    "#---read network and instance data\n",
    "#---nodes and links are sets; links is a set of tuples\n",
    "#---cap, fftt, alpha, beta and cost are dictionaries where the keys are links\n",
    "\n",
    "def read_network_data(net,NDP):\n",
    "    network_data = open(instance_path +'SiouxFalls_DNDP_instances/' + net+NDP+'.txt','r')\n",
    "    lines_net = network_data.readlines()\n",
    "    network_data.close()\n",
    "    nb_nodes = int(lines_net[1].split(\"\\t\")[0].split(\" \")[3])\n",
    "    nb_links = int(lines_net[3].split(\"\\t\")[0].split(\" \")[3])\n",
    "    nb_links2 = int(lines_net[4].split(\"\\t\")[0].split(\" \")[4])\n",
    "    cap = {};fftt = {};alpha = {};beta = {};cost = {};\n",
    "    nodes = set();links = set();links2 = set();\n",
    "\n",
    "    #---offset is network-specific\n",
    "    if net == 'SF':\n",
    "        offset = 9\n",
    "    for i in range(offset,offset+nb_links):\n",
    "        a = int(lines_net[i].split(\"\\t\")[1])\n",
    "        b = int(lines_net[i].split(\"\\t\")[2])\n",
    "        cap[(a,b)] = float(lines_net[i].split(\"\\t\")[3])\n",
    "        fftt[(a,b)] = float(lines_net[i].split(\"\\t\")[5])\n",
    "        alpha[(a,b)] = float(lines_net[i].split(\"\\t\")[6])\n",
    "        beta[(a,b)] = float(lines_net[i].split(\"\\t\")[7])\n",
    "        nodes.add(a)\n",
    "        nodes.add(b)\n",
    "        links.add((a,b))\n",
    "    for i in range(offset+nb_links,offset+nb_links+nb_links2):\n",
    "        a = int(lines_net[i].split(\"\\t\")[1])\n",
    "        b = int(lines_net[i].split(\"\\t\")[2])\n",
    "        cap[(a,b)] = float(lines_net[i].split(\"\\t\")[3])\n",
    "        fftt[(a,b)] = float(lines_net[i].split(\"\\t\")[5])\n",
    "        alpha[(a,b)] = float(lines_net[i].split(\"\\t\")[6])\n",
    "        beta[(a,b)] = float(lines_net[i].split(\"\\t\")[7])\n",
    "        cost[(a,b)] = float(lines_net[i].split(\"\\t\")[11])\n",
    "        nodes.add(a)\n",
    "        nodes.add(b)\n",
    "        links2.add((a,b))\n",
    "    return nodes,links,cap,fftt,alpha,beta,links2,cost\n",
    "\n",
    "\n",
    "#---read trip data\n",
    "#---OD_demand is a dictionary where the keys are OD pairs and the values are the demands\n",
    "# instance_path = '/content/drive/My Drive/Bilevel_data/'\n",
    "def read_trip_data(net):\n",
    "    trip_data = open(instance_path + 'SiouxFalls_TNTP/' + net+'_trips.txt','r')\n",
    "    trip_lines = trip_data.readlines()\n",
    "    trip_data.close()\n",
    "    nb_zones = int(trip_lines[0].split(\"\\t\")[0].split(\" \")[3])\n",
    "    total_flow = float(trip_lines[1].split(\"\\t\")[0].split(\" \")[3])\n",
    "    dest = 0\n",
    "    line_nb = 0\n",
    "    OD_demand = {}\n",
    "    O = set()\n",
    "    D = set()\n",
    "    for line in trip_lines:\n",
    "        if line_nb>=5:\n",
    "            if line.split(\" \")[0]==\"Origin\":\n",
    "                orig = int(line.split(\" \")[1])\n",
    "                orig_flow = 0\n",
    "            elif len(line.split())>0:\n",
    "                k=0\n",
    "                bouh = int(len(line.split())/3)\n",
    "                for i in range(bouh):\n",
    "                    dest = int(line.split()[k])\n",
    "                    demand = float(line.split()[k+2].split(\";\")[0])\n",
    "                    orig_flow += demand\n",
    "                    k += 3\n",
    "                    OD_demand[(orig,dest)] = demand\n",
    "                    O.add(orig)\n",
    "                    D.add(dest)\n",
    "        line_nb += 1\n",
    "    return OD_demand,O,D\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O51RCjWVtRYX"
   },
   "outputs": [],
   "source": [
    "#---solve TAP as a convex NLP using Pyomo and IPOPT for a given y-vector\n",
    "def TAP_cvx(data,yopt):\n",
    "    time0 = time.time()\n",
    "    N = data['nodes']\n",
    "    A = data['links']\n",
    "    A1 = data['links1']\n",
    "    A2 = data['links2']\n",
    "    D = data['dest']\n",
    "    T = data['fftt']\n",
    "    c = data['coef']\n",
    "    e = data['exp']\n",
    "    d = data['demand']\n",
    "    Mflow = data['Mflow']\n",
    "    Mtt = data['Mtt']\n",
    "\n",
    "    TAP = pyo.ConcreteModel()\n",
    "    TAP.x = pyo.Var([(i,j) for (i,j) in A],domain=pyo.NonNegativeReals)\n",
    "    TAP.xc = pyo.Var([(i,j) for (i,j) in A],[s for s in D],domain=pyo.NonNegativeReals)\n",
    "\n",
    "    TAP.cons = pyo.ConstraintList()\n",
    "    for i in N:\n",
    "        for s in D:\n",
    "            TAP.cons.add(sum(TAP.xc[i,j,s] for j in N if (i,j) in A) - sum(TAP.xc[j,i,s] for j in N if (j,i) in A) == d[i,s])\n",
    "\n",
    "    TAP.flow = pyo.ConstraintList()\n",
    "    for (i,j) in A:\n",
    "        TAP.flow.add(sum(TAP.xc[i,j,s] for s in D) == TAP.x[i,j])\n",
    "\n",
    "    TAP.dsgn = pyo.ConstraintList()\n",
    "    for (i,j) in A2:\n",
    "        TAP.dsgn.add(TAP.x[i,j] <= yopt[i,j]*Mflow)\n",
    "\n",
    "    TAP.obj = pyo.Objective(expr=(sum(TAP.x[i,j]*T[i,j] + c[i,j]/(e[i,j]+1)*((TAP.x[i,j])**(e[i,j]+1)) for (i,j) in A)))\n",
    "\n",
    "    opt = SolverFactory('ipopt')#, executable='ipopt/')\n",
    "    results = opt.solve(TAP)\n",
    "    \n",
    "    time_cvx = time.time() - time0\n",
    "\n",
    "    if (results.solver.status == SolverStatus.ok) and (results.solver.termination_condition == TerminationCondition.optimal):\n",
    "        x_cvx = {}\n",
    "        t_cvx = {}\n",
    "        TSTT_cvx = 0\n",
    "        for (i,j) in A:\n",
    "            x_cvx[i,j] = pyo.value(TAP.x[i,j])\n",
    "            if (i,j) in A2 and yopt[i,j] < 1e-4:\n",
    "                t_cvx[i,j] = Mtt\n",
    "            else:\n",
    "                t_cvx[i,j] = T[i,j] + c[i,j]*(x_cvx[i,j]**(e[i,j]))\n",
    "            TSTT_cvx += x_cvx[i,j]*T[i,j] + c[i,j]*(x_cvx[i,j]**(e[i,j]+1))\n",
    "        LL_obj = pyo.value(TAP.obj)\n",
    "        return TSTT_cvx, LL_obj, time_cvx, x_cvx\n",
    "\n",
    "    elif (results.solver.termination_condition == TerminationCondition.infeasible):\n",
    "        print('infeasible',results.solver.status,results.solver.termination_condition)\n",
    "        return -1,-1,time_cvx,{}\n",
    "    else:\n",
    "        print('Solver Status',results.solver.status,results.solver.termination_condition)\n",
    "        return -1,-1,time_cvx,{}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_projects():\n",
    "\n",
    "    net = 'SF'\n",
    "    attributes = ['fftt','alpha_raw','beta','capa','exp','cost']\n",
    "    all_data = {attribute:{} for attribute in attributes}\n",
    "    all_projects = set()\n",
    "    # iteration over the 10 20-link instances only\n",
    "    for i in range(2,3):\n",
    "        for j in range(1,11):\n",
    "            NDP = '_DNDP_'+str(i*10)+'_'+str(j)\n",
    "            print('instance',NDP)\n",
    "            \n",
    "            #---read instance data\n",
    "            data = read_instance(net,NDP,0,100,1e-0,1e-3,600)\n",
    "            \n",
    "            A2 = data['links2']\n",
    "            all_projects.update(A2)\n",
    "            for attribute in attributes:\n",
    "                all_data[attribute].update(data[attribute])\n",
    "\n",
    "    all_projects_list = list(all_projects)\n",
    "    project_mapping = {all_projects_list[i]:i for i in range(len(all_projects_list))}\n",
    "\n",
    "    return all_data, project_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MKKT Baseline Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U9VVI6Vs88AA"
   },
   "outputs": [],
   "source": [
    "def model_MKKT_gurobi(data, timelimit_grb=60):\n",
    "    print('model_MKKT_gurobi timelimit', timelimit_grb)\n",
    "\n",
    "    MKKT = gp.Model()\n",
    "    N = data['nodes']\n",
    "    A = data['links']\n",
    "    A1 = data['links1']\n",
    "    A2 = data['links2']\n",
    "    D = data['dest']\n",
    "    V = data['approx']\n",
    "    T = data['fftt']\n",
    "    c = data['coef']\n",
    "    e = data['exp']\n",
    "    a = data['alpha']\n",
    "    g = data['cost']\n",
    "    B = data['budget']\n",
    "    d = data['demand']\n",
    "    Mflow = data['Mflow']\n",
    "    Mtt = data['Mtt']\n",
    "    timelimit = data['timelimit']\n",
    "    Vp = V.difference({0})\n",
    "\n",
    "    #---primal follower variables\n",
    "    A_lists = list(zip(*A))\n",
    "    A2_lists = list(zip(*A2))\n",
    "    indices_AD = [(i,j,s) for (i,j) in A for s in D]\n",
    "    indices_AV = [(i,j,v) for (i,j) in A for v in V]\n",
    "    indices_ND = [(i,s) for i in N for s in D]\n",
    "    indices_A = list(zip(A_lists[0], A_lists[1]))\n",
    "    indices_A2 = list(zip(A2_lists[0], A2_lists[1]))\n",
    "\n",
    "    xc = MKKT.addVars(indices_AD)\n",
    "    ll = MKKT.addVars(indices_AV)\n",
    "    lr = MKKT.addVars(indices_AV)\n",
    "\n",
    "    #---dual follower variables\n",
    "    pi = MKKT.addVars(indices_ND)\n",
    "    beta = MKKT.addVars(indices_A)\n",
    "    gamma = MKKT.addVars(indices_A)\n",
    "    mu = MKKT.addVars(indices_A2)\n",
    "\n",
    "    #---leader and linearization variables\n",
    "    y = MKKT.addVars(indices_A2, vtype=gp.GRB.BINARY)\n",
    "    phi = MKKT.addVars(indices_A2)\n",
    "\n",
    "    MKKT.addConstr(sum(y[i,j]*g[i,j] for (i,j) in A2) <= B)\n",
    "\n",
    "    for i in N:\n",
    "        for s in D:\n",
    "            MKKT.addConstr(sum(xc[i,j,s] for j in N if (i,j) in A)\n",
    "                               - sum(xc[j,i,s] for j in N if (j,i) in A) == d[i,s])\n",
    "    for (i,j) in A:\n",
    "        MKKT.addConstr(sum(xc[i,j,s] for s in D) == sum(ll[i,j,v]*a[i,j,v-1] + lr[i,j,v]*a[i,j,v] for v in Vp))\n",
    "        MKKT.addConstr(sum(ll[i,j,v] + lr[i,j,v] for v in V) == 1)\n",
    "\n",
    "    for (i,j) in A2:\n",
    "        for s in D:\n",
    "            MKKT.addConstr(xc[i,j,s] <= y[i,j]*Mflow)\n",
    "\n",
    "    for (i,j) in A1:\n",
    "        for s in D:\n",
    "            MKKT.addConstr(pi[i,s] - pi[j,s] + beta[i,j] >= - T[i,j])\n",
    "    for (i,j) in A2:\n",
    "        for s in D:\n",
    "            MKKT.addConstr(pi[i,s] - pi[j,s] + beta[i,j] + mu[i,j] >= - T[i,j])\n",
    "    for (i,j) in A:\n",
    "        for v in Vp:\n",
    "            MKKT.addConstr(- beta[i,j]*a[i,j,v] + gamma[i,j] >= - (c[i,j]/(e[i,j]+1))*(a[i,j,v]**(e[i,j]+1)))\n",
    "        MKKT.addConstr(gamma[i,j] >= 0)\n",
    "\n",
    "    for (i,j) in A2:\n",
    "        MKKT.addConstr(phi[i,j] <= mu[i,j])\n",
    "        MKKT.addConstr(phi[i,j] >= mu[i,j] - (1 - y[i,j])*Mtt)\n",
    "        MKKT.addConstr(phi[i,j] <= y[i,j]*Mtt)\n",
    "        MKKT.addConstr(phi[i,j] >= 0)\n",
    "\n",
    "    # primal-dual constraint\n",
    "    MKKT.addConstr(sum(T[i,j]*sum(xc[i,j,s] for s in D)\n",
    "                           + (c[i,j]/(e[i,j]+1))*sum(ll[i,j,v]*(a[i,j,v-1]**(e[i,j]+1)) + lr[i,j,v]*(a[i,j,v]**(e[i,j]+1))\n",
    "                                                     for v in Vp) for (i,j) in A)\n",
    "                       <= -(sum(pi[i,s]*d[i,s] for i in N for s in D)\n",
    "                           + sum(gamma[i,j] for (i,j) in A) + sum(phi[i,j]*Mflow for (i,j) in A2)))\n",
    "\n",
    "    for (i,j) in A:\n",
    "        for s in D:\n",
    "            MKKT.addConstr(xc[i,j,s] >= 0)\n",
    "        for v in V:\n",
    "            MKKT.addConstr(ll[i,j,v] >= 0)\n",
    "            MKKT.addConstr(lr[i,j,v] >= 0)\n",
    "    for (i,j) in A2:\n",
    "        MKKT.addConstr(mu[i,j] >= 0)\n",
    "\n",
    "    MKKT.setObjective(sum(T[i,j]*sum(xc[i,j,s] for s in D) for (i,j) in A)\n",
    "                 + sum(c[i,j]*sum(ll[i,j,v]*(a[i,j,v-1]**(e[i,j]+1)) + lr[i,j,v]*(a[i,j,v]**(e[i,j]+1))\n",
    "                                  for v in Vp) for (i,j) in A))\n",
    "\n",
    "    MKKT.Params.MIPFocus = 1\n",
    "    MKKT.Params.TimeLimit = timelimit_grb\n",
    "    MKKT.Params.OutputFlag = 0\n",
    "\n",
    "    MKKT.optimize()\n",
    "\n",
    "    time_MKKT = MKKT.Runtime\n",
    "    if MKKT.Status == 'INFEASIBLE' or MKKT.SolCount == 0:\n",
    "        print('status\\t%s' % MKKT.Status)\n",
    "        return -1,-1,time_MKKT,{},{}\n",
    "\n",
    "    UB_MKKT = MKKT.getObjective().getValue()\n",
    "    gap_MKKT = MKKT.MIPGap\n",
    "    print('\\n---MKKT-------------------------------------')\n",
    "    print('status\\t%s' % MKKT.Status)\n",
    "    print('time\\t%.2f' % time_MKKT)\n",
    "    print('OPT\\t%.3f, GAP\\t%.3f' % (UB_MKKT, gap_MKKT))\n",
    "    yopt = {(i,j):y[i,j].X for (i,j) in A2}\n",
    "    # xcopt = {(i,j,s):xc[i,j,s].X for (i,j) in A for s in D}\n",
    "    # xopt = {(i,j):sum(xcopt[i,j,s] for s in D) for (i,j) in A}\n",
    "    # xopt = {(i,j):sum(xcopt[i,j,s] for s in D) for (i,j) in A}\n",
    "    return UB_MKKT,gap_MKKT,time_MKKT,yopt,None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_data, project_mapping = get_projects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UPKGDpN4tUdz",
    "outputId": "ce9e0466-2ea3-4618-d019-4a4e29ff2490"
   },
   "outputs": [],
   "source": [
    "# definte network\n",
    "net = 'SF'\n",
    "\n",
    "# number of samples for train/test sets\n",
    "num_samples = {'train' : 1000, 'test' : 100}\n",
    "\n",
    "x_all = {}\n",
    "y_ul_all = {}\n",
    "y_ll_all = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time_data = time.time()\n",
    "\n",
    "for mode in ['train', 'test']:\n",
    "    x_all[mode] = np.zeros((num_samples[mode], len(project_mapping)))\n",
    "    y_ul_all[mode] = np.zeros(num_samples[mode])\n",
    "    y_ll_all[mode] = np.zeros(num_samples[mode])\n",
    "    \n",
    "    for i in range(num_samples[mode]):\n",
    "        NDP = '_DNDP_10_1'\n",
    "        \n",
    "        num_edges_added = random.randint(1,20)\n",
    "        \n",
    "        my_edges = random.sample(list(project_mapping.keys()), num_edges_added)\n",
    "        print(my_edges)\n",
    "        \n",
    "        #---read instance data\n",
    "        data = read_instance(net,NDP,0,100,1e-0,1e-3,600, my_edges=my_edges, my_data=all_data)\n",
    "        \n",
    "        #---get total cost\n",
    "        TC = sum(data['cost'][i,j] for (i,j) in data['cost'])\n",
    "        \n",
    "        #--- 1 level of budget: 25% of TC\n",
    "        for k in range(1,2):\n",
    "            data['budget'] = TC*k/4\n",
    "            print('>>> NDP',i,100*k/4)\n",
    "            # UB_MKKT,gap_MKKT,time_MKKT,y_MKKT,x_MKKT = model_MKKT_gurobi(data)\n",
    "        \n",
    "        #---determine true TSTT using convex local solver\n",
    "        y_MKKT = {(i,j):1 for (i,j) in my_edges}\n",
    "        TSTT_cvx, LL_obj, time_cvx, x_cvx = TAP_cvx(data,y_MKKT)\n",
    "        \n",
    "        x_in = np.zeros(len(project_mapping))\n",
    "        x_in[[project_mapping[edge] for edge in my_edges]] = 1\n",
    "        y_out = TSTT_cvx\n",
    "        \n",
    "        x_all[mode][i] = x_in\n",
    "        y_ul_all[mode][i] = y_out\n",
    "        y_ll_all[mode][i] = LL_obj\n",
    "\n",
    "time_data = time.time() - time_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data collection time:\", time_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "data = {'x' : x_all, 'y_ul_all' : y_ul_all, 'y_ll_all' : y_ll_all, 'time' : time_data}\n",
    "\n",
    "with open('data/ml_data.pkl', 'wb') as p:\n",
    "    pkl.dump(data, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vylGpCgcR25A"
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "with open(f'{data_path}ml_data.pkl', 'rb') as p:\n",
    "    data = pkl.load(p)\n",
    "\n",
    "x_all = data['x']\n",
    "y_ul_all = data['y_ul_all']\n",
    "y_ll_all = data['y_ll_all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data['y_ul_all']['train'], bins=20)\n",
    "plt.title(\"Upper-Level Objectives\")\n",
    "plt.xlabel(\"Objective\")\n",
    "plt.ylabel(\"Freq\")\n",
    "plt.savefig(f\"{fig_path}/label_dist_upper.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data['y_ll_all']['train'], bins=20)\n",
    "\n",
    "plt.title(\"Lower-Level Objectives\")\n",
    "plt.xlabel(\"Objective\")\n",
    "plt.ylabel(\"Freq\")\n",
    "# plt.show()\n",
    "plt.savefig(f\"{fig_path}/label_dist_lower.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ombRU7c6YkL8",
    "outputId": "704bdcac-7d91-4aa7-853e-ee7b67465855"
   },
   "outputs": [],
   "source": [
    "min_max_scaler_ul = preprocessing.MinMaxScaler()\n",
    "min_max_scaler_ul.fit(y_ul_all['train'].reshape(-1, 1))\n",
    "y_ul_all['train'] = min_max_scaler_ul.transform(y_ul_all['train'].reshape(-1, 1))\n",
    "y_ul_all['test'] = min_max_scaler_ul.transform(y_ul_all['test'].reshape(-1, 1))\n",
    "\n",
    "min_max_scaler_ll = preprocessing.MinMaxScaler()\n",
    "min_max_scaler_ll.fit(y_ll_all['train'].reshape(-1, 1))\n",
    "y_ll_all['train'] = min_max_scaler_ll.transform(y_ll_all['train'].reshape(-1, 1))\n",
    "y_ll_all['test'] = min_max_scaler_ll.transform(y_ll_all['test'].reshape(-1, 1))\n",
    "\n",
    "\n",
    "print(\"Upper-level:\", min_max_scaler_ul.data_min_, min_max_scaler_ul.data_max_)\n",
    "print(\"Lower-level:\", min_max_scaler_ll.data_min_, min_max_scaler_ll.data_max_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train GBT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lower-Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1y17BhpL9FaF",
    "outputId": "5ae0887a-7e81-4830-c322-5ed1ab7249bc"
   },
   "outputs": [],
   "source": [
    "time_gbt_ul = time.time()\n",
    "gbt_ul = GradientBoostingRegressor().fit(x_all['train'], y_ul_all['train'])\n",
    "time_gbt_ul = time.time() - time_gbt_ul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "pred_tr = gbt_ul.predict(x_all['train'])\n",
    "pred_te = gbt_ul.predict(x_all['test'])\n",
    "\n",
    "print('GBT train MAE:', mean_absolute_error(pred_tr, y_ul_all['train']))\n",
    "print('GBT test MAE: ', mean_absolute_error(pred_te, y_ul_all['test']))\n",
    "print(\"Time GBT:\", time_gbt_ul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_ul._time = time_gbt_ul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{data_path}gbt_upper.pkl\", \"wb\") as p:\n",
    "    pkl.dump(gbt_ul, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upper-Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_gbt_ll = time.time()\n",
    "gbt_ll = GradientBoostingRegressor().fit(x_all['train'], y_ll_all['train'])\n",
    "time_gbt_ll = time.time() - time_gbt_ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "pred_tr = gbt_ll.predict(x_all['train'])\n",
    "pred_te = gbt_ll.predict(x_all['test'])\n",
    "\n",
    "print('GBT train MAE:', mean_absolute_error(pred_tr, y_ll_all['train']))\n",
    "print('GBT test MAE: ', mean_absolute_error(pred_te, y_ll_all['test']))\n",
    "print(\"Time GBT:\", time_gbt_ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_ll._time = time_gbt_ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{data_path}gbt_lower.pkl\", \"wb\") as p:\n",
    "    pkl.dump(gbt_ll, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i8iJrDdWVdEV"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def test_model_predictions(a_dataset, device=torch.device(\"cuda\"), print_predictions=False, get_ranking=False, mae=False, verbose=True):\n",
    "    test_loader = DataLoader(a_dataset, shuffle=False, batch_size=len(a_dataset))\n",
    "    err = 0\n",
    "    err_max_over = -1\n",
    "    err_max_under = -1\n",
    "    counter = 0\n",
    "    \n",
    "    labels_instance = []\n",
    "    outputs_instance = []\n",
    "    \n",
    "    res_kendall_all = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "      for i,(features,labels_all) in enumerate(test_loader,0):\n",
    "          features = features.reshape(-1,input_size).to(device)\n",
    "          outputs_all = nn_ul(features).to(device)\n",
    "          if print_predictions:\n",
    "              print(labels_all, outputs_all)\n",
    "          labels_all = labels_all.cpu()\n",
    "          outputs_all = outputs_all.cpu()\n",
    "    r2_nn = r2_score(outputs_all.numpy(), labels_all.numpy())\n",
    "    print('r2nn =', r2_nn)\n",
    "    \n",
    "    for i in range(len(labels_all)):\n",
    "      outputs = outputs_all[i]\n",
    "      labels = labels_all[i]\n",
    "      err_cur = np.abs(labels-outputs)/labels if not mae else np.abs(labels-outputs)\n",
    "      err += err_cur\n",
    "      err_max_over = np.max([err_cur, err_max_over]) if outputs - labels > 0 else err_max_over\n",
    "      err_max_under = np.max([err_cur, err_max_under]) if outputs - labels < 0 else err_max_under\n",
    "    \n",
    "      labels_instance += [labels.item()]\n",
    "      outputs_instance += [outputs.item()]\n",
    "    \n",
    "      if get_ranking and (i % num_sample_perinst) == 0 and i > 0:\n",
    "        res_kendall = stats.kendalltau(labels_instance, outputs_instance)\n",
    "        res_kendall_all += [res_kendall.statistic]\n",
    "        \n",
    "        labels_instance = []\n",
    "        outputs_instance = []\n",
    "    \n",
    "      counter += 1\n",
    "        \n",
    "    try:\n",
    "        mape = err/counter\n",
    "    except:\n",
    "        mape = err[0,0].item()/counter\n",
    "    if verbose:\n",
    "        print(\"Mean Percentage Error =\", mape)\n",
    "        print(\"max over/underestimate \\%:\", err_max_over, err_max_under)\n",
    "    if get_ranking:\n",
    "        print(\"Kendall:\", np.min(res_kendall_all), np.median(res_kendall_all), np.max(res_kendall_all))\n",
    "    \n",
    "    print(\"-------------------------------------\")\n",
    "    return mape, err_max_over, err_max_under"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sl-i6pmORL0J"
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, embedding_size, num_layers, output_relu=False):\n",
    "        super(FeedForward, self).__init__()\n",
    "        layers = collections.OrderedDict()\n",
    "        for i in range(num_layers+1):\n",
    "          in_size = input_size if i == 0 else hidden_size\n",
    "          out_size = embedding_size if i == num_layers else hidden_size\n",
    "          layers[str(i)] = nn.Linear(in_size, out_size, bias=True)\n",
    "          if i < num_layers or output_relu:\n",
    "            layers[\"relu\" + str(i)] = nn.ReLU()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.sequential = torch.nn.Sequential(layers)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.sequential(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upper-Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7d2sMWabS3fg",
    "outputId": "2cacb560-d3ad-49fb-b2f9-95ea4dace4ab"
   },
   "outputs": [],
   "source": [
    "# initialize network\n",
    "nn_ul = FeedForward(input_size=len(project_mapping), hidden_size=16, embedding_size=1, num_layers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize dataset\n",
    "dataset = TensorDataset(torch.from_numpy(x_all['train']).float(), torch.from_numpy(y_ul_all['train']).float())\n",
    "dataset_test = TensorDataset(torch.from_numpy(x_all['test']).float(), torch.from_numpy(y_ul_all['test']).float())\n",
    "\n",
    "training_size = len(dataset)\n",
    "input_size = len(dataset[0][0])\n",
    "print(training_size, input_size)\n",
    "\n",
    "batch_size = 64\n",
    "loader = DataLoader(dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "total_size = len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(nn_ul.parameters(), lr=1e-2)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.9, cooldown=100, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "nn_ul.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "nn_ul_time = time.time()\n",
    "\n",
    "num_epochs = 100\n",
    "loss_epoch = []\n",
    "val_mape_min = math.inf\n",
    "loss_epoch_min_idx = 0\n",
    "epoch = 0\n",
    "while epoch < (num_epochs):\n",
    "    loss_epoch += [0]\n",
    "    for i,(features, labels) in enumerate(loader,0):\n",
    "        features = features.reshape(-1,input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward\n",
    "        outputs = nn_ul(features).to(device)\n",
    "        loss = criterion(outputs,labels)\n",
    "        loss_epoch[-1] += loss.item()/training_size\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "   \n",
    "    epoch += 1\n",
    "\n",
    "nn_ul_time = time.time() - nn_ul_time\n",
    "print(\"Training time: \", nn_ul_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MSE final epoch:\", loss_epoch[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "UbG40FY_UEHs",
    "outputId": "6a5eee9e-bf07-4c13-97b5-97d5a07f1865"
   },
   "outputs": [],
   "source": [
    "plt.plot(loss_epoch)\n",
    "plt.title('NN lower-level loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.savefig(f\"{fig_path}/loss_nn_lower.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_ul._time = nn_ul_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_nn = f'{data_path}nn_upper.pt'\n",
    "torch.save(nn_ul, path_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_ul.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lower-Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize network\n",
    "nn_ll = FeedForward(input_size=len(project_mapping), hidden_size=8, embedding_size=1, num_layers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize dataset\n",
    "dataset = TensorDataset(torch.from_numpy(x_all['train']).float(), torch.from_numpy(y_ll_all['train']).float())\n",
    "dataset_test = TensorDataset(torch.from_numpy(x_all['test']).float(), torch.from_numpy(y_ll_all['test']).float())\n",
    "\n",
    "training_size = len(dataset)\n",
    "input_size = len(dataset[0][0])\n",
    "print(training_size, input_size)\n",
    "\n",
    "batch_size = 64\n",
    "loader = DataLoader(dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "total_size = len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(nn_ll.parameters(), lr=1e-2)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.9, cooldown=100, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "nn_ll.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "nn_ll_time = time.time()\n",
    "\n",
    "num_epochs = 100\n",
    "loss_epoch = []\n",
    "val_mape_min = math.inf\n",
    "loss_epoch_min_idx = 0\n",
    "epoch = 0\n",
    "while epoch < (num_epochs):\n",
    "    loss_epoch += [0]\n",
    "    for i,(features, labels) in enumerate(loader,0):\n",
    "        features = features.reshape(-1,input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward\n",
    "        outputs = nn_ll(features).to(device)\n",
    "        loss = criterion(outputs,labels)\n",
    "        loss_epoch[-1] += loss.item()/training_size\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "   \n",
    "    epoch += 1\n",
    "\n",
    "nn_ll_time = time.time() - nn_ll_time\n",
    "\n",
    "print(\"Training time: \", nn_ll_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_epoch)\n",
    "plt.title('NN upper-level loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.savefig(f\"{fig_path}/loss_nn_upper.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_ll._time = nn_ll_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_nn = f'{data_path}nn_lower.pt'\n",
    "torch.save(nn_ll, path_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_ll.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Surrogate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gurobi_model(data, pred_model, model_type, approx_type, project_mapping, nonconvex=-1, timelimit=60):\n",
    "    grb_model = gp.Model()\n",
    "    \n",
    "    N = data['nodes']\n",
    "    A = data['links']\n",
    "    A1 = data['links1']\n",
    "    A2 = data['links2']\n",
    "    D = data['dest']\n",
    "    V = data['approx']\n",
    "    T = data['fftt']\n",
    "    c = data['coef']\n",
    "    e = data['exp']\n",
    "    a = data['alpha']\n",
    "    g = data['cost']\n",
    "    B = data['budget']\n",
    "    d = data['demand']\n",
    "    Mflow = data['Mflow']\n",
    "    Mtt = data['Mtt']\n",
    "    Vp = V.difference({0})\n",
    "    \n",
    "    #---primal follower variables\n",
    "    A_lists = list(zip(*A))\n",
    "    A2_lists = list(zip(*A2))\n",
    "    indices_AD = [(i,j,s) for (i,j) in A for s in D]\n",
    "    indices_AV = [(i,j,v) for (i,j) in A for v in V]\n",
    "    indices_ND = [(i,s) for i in N for s in D]\n",
    "    indices_A = list(zip(A_lists[0], A_lists[1]))\n",
    "    indices_A2 = list(zip(A2_lists[0], A2_lists[1]))\n",
    "    \n",
    "    xc = grb_model.addVars(indices_AD, name=\"xc\")\n",
    "    \n",
    "    #---leader\n",
    "    y = grb_model.addVars(project_mapping.keys(), vtype=gp.GRB.BINARY, name=\"y\")\n",
    "    # leader variable fixed to zero if not candidate edge\n",
    "    for edge, edge_id in project_mapping.items():\n",
    "        if edge not in A2:\n",
    "            grb_model.addConstr(y[edge] == 0)\n",
    "\n",
    "    # leader budget constraint\n",
    "    grb_model.addConstr(gp.quicksum(y[edge] * g[edge] for edge in A2) <= B) # y @ edge_costs <= B)\n",
    "\n",
    "    # add prediction for network/gbt\n",
    "    pred_var = grb_model.addMVar((1,), lb=-gp.GRB.INFINITY, name=\"pred\")\n",
    "    if model_type == \"nn\":\n",
    "        pred_constr = add_predictor_constr(grb_model, pred_model.sequential, y, pred_var)\n",
    "    elif model_type == \"gbt\":\n",
    "        pred_constr = add_gradient_boosting_regressor_constr(grb_model, pred_model, y, pred_var)\n",
    "    else:\n",
    "        raise Exception(f\"model_type ({model_type}) not implemented\")\n",
    "\n",
    "    # add constraints/objective for upper/lower specific approximations\n",
    "    if approx_type == \"lower\":\n",
    "        # follower flow variables, original and to the power 5\n",
    "        x = grb_model.addVars(indices_A, name=\"x\")\n",
    "        x5p = grb_model.addVars(indices_A, name=\"x5p\")\n",
    "\n",
    "        grb_model._x = x\n",
    "        grb_model._x5p = x5p\n",
    "\n",
    "        # neural network slack variable\n",
    "        slack = grb_model.addMVar((1,), name=\"slack\")\n",
    "        \n",
    "        # follower variable to the power 5\n",
    "        for edge in A:\n",
    "            grb_model.addGenConstrPoly(x[edge], x5p[edge], [1, 0, 0, 0, 0, 0], name=f\"poly_{edge}\")\n",
    "        \n",
    "        # follower flow balance constraint\n",
    "        for i in N:\n",
    "            for s in D:\n",
    "                grb_model.addConstr(gp.quicksum(xc[i,j,s] for j in N if (i,j) in A) -\n",
    "                                    gp.quicksum(xc[j,i,s] for j in N if (j,i) in A) == d[i,s])\n",
    "        \n",
    "        # follower edge flow = sum of all flows through edge to all destination nodes\n",
    "        for (i,j) in A:\n",
    "            grb_model.addConstr(gp.quicksum(xc[i,j,s] for s in D) == x[i,j])\n",
    "        \n",
    "        # follower can use edge only if it is chosen by leader\n",
    "        for (i,j) in A2:\n",
    "            grb_model.addConstr(x[i,j] <= y[i,j]*Mflow)\n",
    "\n",
    "        # upper/lower objectives\n",
    "        ul_obj = gp.quicksum(T[i,j]*x[i,j] + c[i,j] * x5p[i,j] for (i,j) in A)\n",
    "        ll_obj = gp.quicksum(T[i,j]*x[i,j] + (c[i,j] / (e[i,j] + 1)) * x5p[i,j] for (i,j) in A)\n",
    "\n",
    "        # constraint to track upper-level objective\n",
    "        ul_obj_var = grb_model.addMVar((1,), name=\"ul_obj\")\n",
    "        grb_model.addConstr(ul_obj_var == ul_obj)\n",
    "        \n",
    "        # scaled prediction\n",
    "        scaler = min_max_scaler_ll\n",
    "        pred_sc = pred_var * (scaler.data_max_ - scaler.data_min_) + scaler.data_min_\n",
    "        \n",
    "        grb_model.setObjective(ul_obj + slack_obj_coef * slack, gp.GRB.MINIMIZE)\n",
    "        grb_model.addConstr(ll_obj <= pred_sc + slack) # justin\n",
    "      \n",
    "    elif approx_type == \"upper\":\n",
    "        grb_model.setObjective(pred_var, gp.GRB.MINIMIZE)\n",
    "\n",
    "    else:\n",
    "        raise Exception(f\"approx_type ({approx_type}) not implemented\")\n",
    "        \n",
    "    grb_model.Params.OutputFlag = 0\n",
    "    grb_model.Params.MIPFocus = 1\n",
    "    grb_model.Params.TimeLimit = timelimit\n",
    "    grb_model.Params.FuncNonlinear = nonconvex\n",
    "    \n",
    "    grb_model.optimize()\n",
    "    \n",
    "    yopt = {(i,j):y[i,j].X for (i,j) in A2}\n",
    "    \n",
    "    return yopt, grb_model.Runtime, grb_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiements with $\\lambda=1$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_timelimit = 5\n",
    "slack_obj_coef = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0nAWXjCTPO2B",
    "outputId": "16738645-a3ba-4353-f6f4-9b72f0ac6d99"
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "#---iterate over 10- and 20-link instance sets\n",
    "for i in range(1,3):\n",
    "\n",
    "    #---iterate over 10 instances\n",
    "    for j in range(1,11):\n",
    "\n",
    "        NDP = '_DNDP_'+str(i*10)+'_'+str(j)\n",
    "        instance_name = 'SF' + NDP\n",
    "        print('instance', instance_name)\n",
    "\n",
    "        #---read instance data\n",
    "        data = read_instance(net,NDP,0,100,1e-0,1e-3,600)\n",
    "\n",
    "        #---get total cost\n",
    "        TC = sum(data['cost'][i,j] for (i,j) in data['cost'])\n",
    "\n",
    "        #---iterate over 3 levels of budget: 25%, 50% and 75% of TC\n",
    "        for k in range(1,4):\n",
    "            data['budget'] = TC*k/4\n",
    "            print('>>> NDP',i*10,j,100*k/4)\n",
    "\n",
    "            mlvariants = [['nn', 'upper', 0],\n",
    "                          ['nn', 'lower', 0],\n",
    "                          ['gbt', 'upper', 0],\n",
    "                          ['gbt', 'lower', 0]]\n",
    "\n",
    "            for mlvariant in mlvariants:\n",
    "                print('mlvariant = ', mlvariant)\n",
    "                if mlvariant[0] == 'nn' and mlvariant[1] == 'upper':\n",
    "                    pred_model = nn_ul\n",
    "                if mlvariant[0] == 'nn' and mlvariant[1] == 'lower':\n",
    "                    pred_model = nn_ll\n",
    "                if mlvariant[0] == 'gbt' and mlvariant[1] == 'upper':\n",
    "                    pred_model = gbt_ul\n",
    "                if mlvariant[0] == 'gbt' and mlvariant[1] == 'lower':\n",
    "                    pred_model = gbt_ll\n",
    "\n",
    "                # solve surrogate ML model\n",
    "                y_ml, time_ml, grb_model = get_gurobi_model(\n",
    "                    data=data, \n",
    "                    pred_model=pred_model, \n",
    "                    model_type=mlvariant[0],\n",
    "                    approx_type=mlvariant[1], \n",
    "                    project_mapping=project_mapping, \n",
    "                    nonconvex=mlvariant[2], \n",
    "                    timelimit=ml_timelimit)\n",
    "                \n",
    "                #---determine true TSTT using convex local solver\n",
    "                trueval_ml, lower_obj_ml, time_cvx_ml, x_cvx_ml = TAP_cvx(data, y_ml)\n",
    "                \n",
    "                print(\"  Upper-level obj: \", trueval_ml)\n",
    "                print(\"  Lower-level obj: \", lower_obj_ml)\n",
    "                \n",
    "                if mlvariant[1] == 'upper':\n",
    "                    scaler = min_max_scaler_ul\n",
    "                    obj_sc = grb_model.objVal * (scaler.data_max_ - scaler.data_min_) + scaler.data_min_\n",
    "                    print(\"  Upper-level pred:\", obj_sc[0])\n",
    "                    print(\"  Pred Gap:        \", 100 * np.abs(trueval_ml - obj_sc[0]) / trueval_ml)\n",
    "\n",
    "                if mlvariant[1] == 'lower':\n",
    "                    pred = grb_model.getVarByName(\"pred[0]\").x\n",
    "                    slack = grb_model.getVarByName(\"slack[0]\").x\n",
    "                    ul_obj_surr = grb_model.getVarByName(\"ul_obj[0]\").x\n",
    "                    \n",
    "                    scaler = min_max_scaler_ll\n",
    "                    pred_sc = pred * (scaler.data_max_ - scaler.data_min_) + scaler.data_min_\n",
    "                    print(\"  Lower-level pred:\", pred_sc[0])\n",
    "                    print(\"  Upper-level surr:\", ul_obj_surr)\n",
    "                    print(\"  Pred Gap:        \", 100 * np.abs(lower_obj_ml - pred_sc) / lower_obj_ml)\n",
    "                    print(\"  Surr Gap:        \", 100 * np.abs(trueval_ml - ul_obj_surr) / trueval_ml)\n",
    "                    print(\"  slack:           \", slack)\n",
    "\n",
    "                print(\"  Time:   \", time_ml)\n",
    "                \n",
    "                results += [[\n",
    "                  instance_name,\n",
    "                  k/4.0,\n",
    "                  i*10,\n",
    "                  '_'.join(str(att) for att in mlvariant),\n",
    "                  trueval_ml,\n",
    "                  lower_obj_ml,\n",
    "                  time_ml\n",
    "                ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ElJqv67sczX_",
    "outputId": "b43296af-174e-4d07-9f9a-5ccd8dcc3b2a"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df.columns = ['instance_name', 'budget', 'n_edges', 'method', 'obj', 'lower_obj', 'time']\n",
    "summary = df.groupby(['budget', 'n_edges', 'method']).agg({'obj': ['mean'], 'time': ['mean']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ILtC-hKxzIQR"
   },
   "outputs": [],
   "source": [
    "df.to_csv(f'{result_path}ml_results_tml-{ml_timelimit}_skl-{slack_obj_coef}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiements with $\\lambda=0.1$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_timelimit = 5\n",
    "slack_obj_coef = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l4x8va3kzppa",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "#---iterate over 10- and 20-link instance sets\n",
    "for i in range(1,3):\n",
    "\n",
    "    #---iterate over 10 instances\n",
    "    for j in range(1,11):\n",
    "\n",
    "        NDP = '_DNDP_'+str(i*10)+'_'+str(j)\n",
    "        instance_name = 'SF' + NDP\n",
    "        print('instance', instance_name)\n",
    "\n",
    "        #---read instance data\n",
    "        data = read_instance(net,NDP,0,100,1e-0,1e-3,600)\n",
    "\n",
    "        #---get total cost\n",
    "        TC = sum(data['cost'][i,j] for (i,j) in data['cost'])\n",
    "\n",
    "        #---iterate over 3 levels of budget: 25%, 50% and 75% of TC\n",
    "        for k in range(1,4):\n",
    "            data['budget'] = TC*k/4\n",
    "            print('>>> NDP',i*10,j,100*k/4)\n",
    "\n",
    "            mlvariants = [['nn', 'lower', 0],\n",
    "                          ['gbt', 'lower', 0]]\n",
    "\n",
    "            for mlvariant in mlvariants:\n",
    "                print('mlvariant = ', mlvariant)\n",
    "                if mlvariant[0] == 'nn' and mlvariant[1] == 'upper':\n",
    "                    pred_model = nn_ul\n",
    "                if mlvariant[0] == 'nn' and mlvariant[1] == 'lower':\n",
    "                    pred_model = nn_ll\n",
    "                if mlvariant[0] == 'gbt' and mlvariant[1] == 'upper':\n",
    "                    pred_model = gbt_ul\n",
    "                if mlvariant[0] == 'gbt' and mlvariant[1] == 'lower':\n",
    "                    pred_model = gbt_ll\n",
    "\n",
    "                # solve surrogate ML model\n",
    "                y_ml, time_ml, grb_model = get_gurobi_model(\n",
    "                    data=data, \n",
    "                    pred_model=pred_model, \n",
    "                    model_type=mlvariant[0],\n",
    "                    approx_type=mlvariant[1], \n",
    "                    project_mapping=project_mapping, \n",
    "                    nonconvex=mlvariant[2], \n",
    "                    timelimit=ml_timelimit)\n",
    "\n",
    "                #---determine true TSTT using convex local solver\n",
    "                trueval_ml, lower_obj_ml, time_cvx_ml, x_cvx_ml = TAP_cvx(data, y_ml)\n",
    "                \n",
    "                print(\"  Upper-level obj: \", trueval_ml)\n",
    "                print(\"  Lower-level obj: \", lower_obj_ml)\n",
    "                \n",
    "                if mlvariant[1] == 'upper':\n",
    "                    scaler = min_max_scaler_ul\n",
    "                    obj_sc = grb_model.objVal * (scaler.data_max_ - scaler.data_min_) + scaler.data_min_\n",
    "                    print(\"  Upper-level pred:\", obj_sc[0])\n",
    "                    print(\"  Pred Gap:        \", 100 * np.abs(trueval_ml - obj_sc[0]) / trueval_ml)\n",
    "\n",
    "                if mlvariant[1] == 'lower':\n",
    "                    pred = grb_model.getVarByName(\"pred[0]\").x\n",
    "                    slack = grb_model.getVarByName(\"slack[0]\").x\n",
    "                    ul_obj_surr = grb_model.getVarByName(\"ul_obj[0]\").x\n",
    "                    \n",
    "                    scaler = min_max_scaler_ll\n",
    "                    pred_sc = pred * (scaler.data_max_ - scaler.data_min_) + scaler.data_min_\n",
    "                    print(\"  Lower-level pred:\", pred_sc[0])\n",
    "                    print(\"  Upper-level surr:\", ul_obj_surr)\n",
    "                    print(\"  Pred Gap:        \", 100 * np.abs(lower_obj_ml - pred_sc) / lower_obj_ml)\n",
    "                    print(\"  Surr Gap:        \", 100 * np.abs(trueval_ml - ul_obj_surr) / trueval_ml)\n",
    "                    print(\"  slack:           \", slack)\n",
    "\n",
    "                print(\"  Time:   \", time_ml)\n",
    "                \n",
    "                results += [[\n",
    "                  instance_name,\n",
    "                  k/4.0,\n",
    "                  i*10,\n",
    "                  '_'.join(str(att) for att in mlvariant),\n",
    "                  trueval_ml,\n",
    "                  lower_obj_ml,\n",
    "                  time_ml\n",
    "                ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df.columns = ['instance_name', 'budget', 'n_edges', 'method', 'obj', 'lower_obj', 'time']\n",
    "summary = df.groupby(['budget', 'n_edges', 'method']).agg({'obj': ['mean'], 'time': ['mean']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'{result_path}ml_results_tml-{ml_timelimit}_skl-{slack_obj_coef}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MKKT Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "#---iterate over 10- and 20-link instance sets\n",
    "for i in range(1,3):\n",
    "\n",
    "    #---iterate over 10 instances\n",
    "    for j in range(1,11):\n",
    "\n",
    "        NDP = '_DNDP_'+str(i*10)+'_'+str(j)\n",
    "        instance_name = 'SF' + NDP\n",
    "        print('instance', instance_name)\n",
    "\n",
    "        #---read instance data\n",
    "        data = read_instance(net,NDP,0,100,1e-0,1e-3,600)\n",
    "\n",
    "        #---get total cost\n",
    "        TC = sum(data['cost'][i,j] for (i,j) in data['cost'])\n",
    "\n",
    "        #---iterate over 3 levels of budget: 25%, 50% and 75% of TC\n",
    "        for k in range(1,4):\n",
    "            data['budget'] = TC*k/4\n",
    "            print('>>> NDP',i*10,j,100*k/4)\n",
    "                \n",
    "            timelimits = [5, 10, 30]\n",
    "            for timelimit in timelimits:\n",
    "                UB_MKKT,gap_MKKT,time_MKKT,y_MKKT,x_MKKT = model_MKKT_gurobi(data, timelimit_grb=timelimit)\n",
    "                \n",
    "                TSTT_cvx, lower_obj_mkkt, time_cvx, x_cvx = TAP_cvx(data,y_MKKT)\n",
    "                \n",
    "                if UB_MKKT == -1:\n",
    "                    TSTT_cvx = -1\n",
    "                else:\n",
    "                    #---determine true TSTT using convex local solver\n",
    "                    TSTT_cvx, lower_obj_cvx, time_cvx, x_cvx = TAP_cvx(data,y_MKKT)\n",
    "\n",
    "                print(\"Timelimit:\", timelimit)\n",
    "                print(\"  Upper-level obj: \", TSTT_cvx)\n",
    "                print(\"  Lower-level obj: \", lower_obj_cvx)\n",
    "                print('  Time (mkkt):     ', time_MKKT )\n",
    "                print('  Time (check):    ', time_cvx)\n",
    "                \n",
    "                results += [[\n",
    "                    instance_name,\n",
    "                    k/4.0,\n",
    "                    i*10,\n",
    "                    'mkkt_%d' % timelimit,\n",
    "                    TSTT_cvx,\n",
    "                    lower_obj_cvx,\n",
    "                    time_MKKT\n",
    "                ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df.columns = ['instance_name', 'budget', 'n_edges', 'method', 'obj', 'lower_obj', 'time']\n",
    "summary = df.groupby(['budget', 'n_edges', 'method']).agg({'obj': ['mean'], 'time': ['mean']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'{result_path}baseline_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPK/RveIQVeCeZSzikhobPE",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
